# ğŸ›¡ï¸ AegisML - AI Model Testing Toolkit

![Build Status](https://github.com/abhishekiitr7/aegisml/actions/workflows/ci.yml/badge.svg)
![Coverage](https://codecov.io/gh/abhishekiitr7/aegisml/branch/main/graph/badge.svg)
![License](https://img.shields.io/github/license/abhishekiitr7/aegisml)
![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)

AegisML is an open-source Python toolkit that automates **testing of AI/ML pipelines** for:
- âœ… **Correctness** â€“ Are outputs as expected?
- âœ… **Reproducibility** â€“ Do results remain consistent across runs?
- âœ… **Performance regression** â€“ Did accuracy drop after code changes?
- âœ… **Robustness** â€“ Does the pipeline handle malformed or extreme inputs?

This framework is designed for data scientists, ML engineers, and AI researchers who need **reliable, maintainable test suites** for complex machine learning systems.

---

## ğŸ“‚ Project Structure

```
aegisml/
â”‚â”€â”€ aegisml/
â”‚   â”œâ”€â”€ config.py              # Global settings
â”‚   â”œâ”€â”€ utils.py               # Data loading & preprocessing
â”‚   â”œâ”€â”€ model_checks.py        # Unit tests for model methods
â”‚   â”œâ”€â”€ integration_tests.py   # End-to-end inference checks
â”‚
â”‚â”€â”€ examples/
â”‚   â”œâ”€â”€ example_pipeline.py    # Sample ML workflow
â”‚
â”‚â”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_checks.py    # Data preprocessing tests
â”‚   â”œâ”€â”€ test_integration.py    # Integration tests
â”‚   â”œâ”€â”€ test_property_based.py # Hypothesis-based stability tests
â”‚   â”œâ”€â”€ test_regression.py     # Accuracy regression tests
â”‚   â”œâ”€â”€ test_fuzz.py           # Robustness fuzz tests
â”‚
â”‚â”€â”€ .github/workflows/ci.yml   # GitHub Actions CI/CD pipeline
â”‚â”€â”€ requirements.txt           # Dependencies
â”‚â”€â”€ setup.py                   # Packaging setup
â”‚â”€â”€ LICENSE
â”‚â”€â”€ README.md
```

---

## Quickstart

### 1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run tests
```bash
pytest --cov=aegisml
```

### 3ï¸âƒ£ Check coverage report
```bash
coverage html
open htmlcov/index.html
```

---

## ğŸ§ª Types of Tests

### **1. Unit Tests for Data Preprocessing**
Ensures preprocessing handles missing values, scaling, and transformations correctly.

### **2. Integration Tests**
Validates the **entire ML pipeline** from raw input â†’ prediction output.

### **3. Property-Based Tests** (`hypothesis`)
Generates random valid data to ensure stability under varied inputs.

### **4. Regression Tests**
Compares new model performance against stored **baseline metrics**.

### **5. Fuzz Tests**
Pushes corrupted/extreme inputs to check robustness.

---

## ğŸ”„ Continuous Integration (CI/CD)
Every push or pull request triggers:
- âœ… Automated tests via **pytest**
- âœ… Coverage reporting via **Codecov**
- âœ… Status badges updated in README

---

## ğŸ“Š Example Workflow

1. **Modify the model** in `model_checks.py`
2. **Run tests**:
```bash
pytest
```
3. If performance drops below threshold â†’ regression test fails.
4. Fix issues before merging.

---

## ğŸ† Why This Project Stands Out
- Covers **entire AI pipeline** testing.
- Combines **classical testing** + **ML-specific checks**.
- Uses **Hypothesis** for advanced property-based testing.
- Built with **professional CI/CD integration**.

---

## ğŸ¤ Contributing
Pull requests are welcome! For major changes, please open an issue first to discuss what you would like to change.

---

## ğŸ“¬ Contact
**Author:** Abhishek Pal  
**GitHub:** [abhishekiitr7](https://github.com/abhishekiitr7)  
**Email:** abhishekiitr7@gmail.com

